{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13080fdd",
   "metadata": {},
   "source": [
    "# Deterministic SSR Peak Caller (Geometric) \n",
    "\n",
    "This notebook implements a **deterministic**, **non‑ML** peak caller for capillary electrophoresis (CE) fragment analysis (SSR genotyping).\n",
    "\n",
    "**Inputs**\n",
    "- `molecular_weight_data.zip`: per-sample CSVs (`M1_plXX.csv`, `M2_plXX.csv`) with `molw` and `channel_1..channel_5`.\n",
    "- `lecture_microsat.zip` (optional): ground-truth allele calls for validation.\n",
    "\n",
    "**Outputs**\n",
    "- `outputs_v20_4/tables/miRNA_allele_calls_wide.csv`\n",
    "- `outputs_v20_4/tables/miRNA_allele_calls_long.csv`\n",
    "- `outputs_v20_4/relatorio_validacao/*` (validation reports; M1 only)\n",
    "\n",
    "> Note on homozygotes: by default, if only one allele peak is called, the exporter duplicates it as (A,A) to match common fragment-analysis exports.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e13bab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import zipfile\n",
    "import concurrent.futures\n",
    "import multiprocessing\n",
    "import math\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks, savgol_filter\n",
    "from scipy.interpolate import PchipInterpolator\n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import spsolve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7e18e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except Exception:\n",
    "    def tqdm(x, **kwargs):\n",
    "        return x\n",
    "\n",
    "\n",
    "# CONFIGURATION & TOGGLES\n",
    "\n",
    "RUN_PARALLEL = False\n",
    "MAX_WORKERS = max(1, multiprocessing.cpu_count() - 1) if RUN_PARALLEL else 1\n",
    "\n",
    "# Paths (mantém a mesma estrutura de saída)\n",
    "ZIP_PATH = Path(\"molecular_weight_data.zip\")\n",
    "LABEL_DIR = Path(\"lecture_microsat\")\n",
    "OUT_DIR = Path(\"outputs_vclassification\")\n",
    "TABLES_DIR = OUT_DIR / \"tables\"\n",
    "PLOTS_DIR = OUT_DIR / \"plots_per_plant\"\n",
    "PARAMS_JSON = OUT_DIR / \"master_panel.json\"\n",
    "\n",
    "GENERATE_PLOTS = True\n",
    "\n",
    "\n",
    "\n",
    "DUPLICATE_HOMOZYGOUS = True\n",
    "HOMO_AS_TWO_ALLELES = True\n",
    "\n",
    "\n",
    "# Signal rules\n",
    "X_MIN_ALLELES = 100.0\n",
    "X_MAX = 520.0\n",
    "\n",
    "X_MIN_LADDER = 30.0\n",
    "\n",
    "SMOOTH_WINDOW = 31\n",
    "SMOOTH_POLYORDER = 3\n",
    "\n",
    "# LADDER LIZ 500 (16 peaks)\n",
    "LIZ_500_SIZES = np.array([35.0, 50.0, 75.0, 100.0, 139.0, 150.0, 160.0, 200.0,\n",
    "                          250.0, 300.0, 340.0, 350.0, 400.0, 450.0, 490.0, 500.0])\n",
    "\n",
    "# HEURISTIC THRESHOLDS \n",
    "MIN_PROM_ABS = 30.0\n",
    "MIN_HEIGHT_ABS = 30.0\n",
    "PROM_SNR = 3.0\n",
    "MIN_PEAK_WIDTH_SAMPLES = 1.0\n",
    "\n",
    "# Crosstalk / pull-up heuristic \n",
    "CROSSTALK_TOL_PB = 0.6\n",
    "CROSSTALK_STRONG_RATIO = 4.0\n",
    "\n",
    "# Painel mestre / snap\n",
    "PANEL_SNAP_TOL_PB = 2.0\n",
    "MIN_ALLELE_SEP_PB = 0.8\n",
    "ALLELE2_MIN_RATIO = 0.15\n",
    "\n",
    "# Stutter: aplicado com proteção para heterozigotos muito próximos\n",
    "STUTTER_WINDOW = (-3.5, 1.0)\n",
    "STUTTER_MAX_RATIO = 0.35\n",
    "\n",
    "# Anti-artifacts \n",
    "# Penalizes spikes that appear simultaneously on multiple channels \n",
    "MULTI_RATIO_MAX = 0.25          \n",
    "MULTI_RATIO_HARD = 0.60         \n",
    "MULTI_WINDOW_SAMPLES = 5        \n",
    "\n",
    "# Penalizes spikes aligned with strong LADDER spikes (channel 5)\n",
    "LADDER_STRONG_Q = 0.90          \n",
    "LADDER_BLEED_RATIO_MAX = 0.10   \n",
    "\n",
    "HET_PROTECT_PB = 1.2  # avoids erasing real alleles ~1 pb\n",
    "\n",
    "# mapping markers\n",
    "MARKER_MAP = {\n",
    "    1: (1, 1), 2: (1, 2), 3: (1, 3), 4: (1, 4),\n",
    "    5: (2, 1), 6: (2, 2), 7: (2, 3), 8: (2, 4),\n",
    "}\n",
    "\n",
    "# DATA STRUCTURES\n",
    "\n",
    "@dataclass\n",
    "class PeakCandidate:\n",
    "    pos: float         \n",
    "    raw_pos: float      \n",
    "    prom: float\n",
    "    height: float\n",
    "\n",
    "@dataclass\n",
    "class MappedPeak:\n",
    "    allele_size: float\n",
    "    calibrated_pos: float  \n",
    "    raw_pos: float        \n",
    "    prom: float\n",
    "    height: float\n",
    "    score: float = 0.0\n",
    "    multi_ratio: float = 0.0\n",
    "    ladder_ratio: float = 1.0\n",
    "    flags: str = \"\"\n",
    "\n",
    "# UTILITIES\n",
    "\n",
    "def safe_savgol(y: np.ndarray, window: int, poly: int) -> np.ndarray:\n",
    "    if window >= len(y):\n",
    "        window = max(5, len(y)//2*2+1)\n",
    "    if window % 2 == 0:\n",
    "        window += 1\n",
    "    if window < poly + 2:\n",
    "        poly = max(1, window - 2)\n",
    "    return savgol_filter(y, window, poly, mode=\"interp\")\n",
    "\n",
    "def baseline_als(y: np.ndarray, lam: float = 1e6, p: float = 0.001, niter: int = 10) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Baseline correction via Asymmetric Least Squares (Eilers & Boelens, 2005).\n",
    "    \"\"\"\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    L = y.size\n",
    "    if L < 5:\n",
    "        return np.zeros_like(y)\n",
    "\n",
    "    D = sparse.diags([1, -2, 1], [0, 1, 2], shape=(L-2, L))\n",
    "    w = np.ones(L)\n",
    "    for _ in range(niter):\n",
    "        W = sparse.spdiags(w, 0, L, L)\n",
    "        Z = W + lam * (D.T @ D)\n",
    "        z = spsolve(Z, w*y)\n",
    "        w = p * (y > z) + (1 - p) * (y <= z)\n",
    "    return z\n",
    "\n",
    "def estimate_noise_mad(y: np.ndarray) -> float:\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    med = np.median(y)\n",
    "    mad = np.median(np.abs(y - med))\n",
    "    return 1.4826 * mad + 1e-9\n",
    "\n",
    "def get_zip_base_dir(zf: zipfile.ZipFile) -> str:\n",
    "    names = [n for n in zf.namelist() if n.endswith(\".csv\") and \"MACOSX\" not in n]\n",
    "    if not names:\n",
    "        return \"\"\n",
    "    # assume base dir is prefix before first file name\n",
    "    parts = names[0].split(\"/\")\n",
    "    return parts[0] if len(parts) > 1 else \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcd964a",
   "metadata": {},
   "source": [
    "## I/O utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "816205b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_plants_in_zip(zip_path: Path) -> List[int]:\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "        base = get_zip_base_dir(zf)\n",
    "        pat = re.compile(rf\"^{re.escape(base) + '/' if base else ''}M1_pl(\\d+)\\.csv$\")\n",
    "        plants = []\n",
    "        for name in zf.namelist():\n",
    "            m = pat.match(name)\n",
    "            if m:\n",
    "                plants.append(int(m.group(1)))\n",
    "        plants.sort()\n",
    "        return plants\n",
    "\n",
    "# MASTER PANEL (binless)\n",
    "\n",
    "def load_ground_truth_labels(label_dir: Path) -> Dict[int, Dict[int, Tuple[Optional[float], Optional[float]]]]:\n",
    "    \"\"\"\n",
    "    Returns labels[plant_id][marker_id] = (a1, a2) as float.\n",
    "    \"\"\"\n",
    "    file_map = {\n",
    "        1: \"resM1P1.csv\", 2: \"resM1P2.csv\", 3: \"resM1P3.csv\", 4: \"resM1P4.csv\",\n",
    "        5: \"resM2P1.csv\", 6: \"resM2P2.csv\", 7: \"resM2P3.csv\", 8: \"resM2P4.csv\",\n",
    "    }\n",
    "    labels = {}\n",
    "    for m_id, fname in file_map.items():\n",
    "        fpath = label_dir / fname\n",
    "        if not fpath.exists():\n",
    "            continue\n",
    "        df = pd.read_csv(fpath, sep=\";\")\n",
    "        df.columns = [c.strip() for c in df.columns]\n",
    "        if \"plant\" not in df.columns:\n",
    "            continue\n",
    "        for _, row in df.iterrows():\n",
    "            pid_val = pd.to_numeric(row[\"plant\"], errors=\"coerce\")\n",
    "            if pd.isna(pid_val):\n",
    "                continue\n",
    "            pid = int(pid_val)\n",
    "            a1 = row.get(\"markA.1\", np.nan)\n",
    "            a2 = row.get(\"markA.2\", np.nan)\n",
    "            a1 = None if pd.isna(a1) else float(a1)\n",
    "            a2 = None if pd.isna(a2) else float(a2)\n",
    "            labels.setdefault(pid, {})[m_id] = (a1, a2)\n",
    "    return labels\n",
    "\n",
    "def load_and_build_panel(label_dir: Path) -> Dict[int, List[float]]:\n",
    "    labels = load_ground_truth_labels(label_dir)\n",
    "    panel = {m: set() for m in range(1, 9)}\n",
    "    for pid, md in labels.items():\n",
    "        for m, (a1, a2) in md.items():\n",
    "            if a1 is not None: panel[m].add(float(a1))\n",
    "            if a2 is not None: panel[m].add(float(a2))\n",
    "    panel = {m: sorted(list(s)) for m, s in panel.items()}\n",
    "    OUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "    with open(PARAMS_JSON, \"w\") as f:\n",
    "        json.dump({str(k): v for k, v in panel.items()}, f, indent=2)\n",
    "    return panel\n",
    "\n",
    "# SUBPIXEL PEAK POSITION\n",
    "\n",
    "def fractional_peak_pos(x: np.ndarray, y: np.ndarray, idx: int) -> float:\n",
    "    \"\"\"\n",
    "    Quadratic fit at 3 points (idx-1, idx, idx+1) for subpixel.\n",
    "    \"\"\"\n",
    "    if idx <= 0 or idx >= len(y) - 1:\n",
    "        return float(x[idx])\n",
    "    x1, x2, x3 = float(x[idx-1]), float(x[idx]), float(x[idx+1])\n",
    "    y1, y2, y3 = float(y[idx-1]), float(y[idx]), float(y[idx+1])\n",
    "\n",
    "    denom = (y1 - 2*y2 + y3)\n",
    "    if abs(denom) < 1e-12:\n",
    "        return x2\n",
    "    delta = 0.5*(y1 - y3)/denom\n",
    "    return x2 + delta*(x3 - x2)\n",
    "\n",
    "# LADDER CALIBRATION \n",
    "\n",
    "def _detect_ladder_peaks(raw_x: np.ndarray, y5: np.ndarray) -> List[Tuple[float, float, float]]:\n",
    "    \"\"\"\n",
    "     Detects peaks on channel 5 (ladder), returning a list (raw_pos, prom, height).\n",
    "    \"\"\"\n",
    "    mask = (raw_x >= X_MIN_LADDER) & (raw_x <= X_MAX)\n",
    "    x = raw_x[mask]\n",
    "    y = y5[mask].astype(float)\n",
    "\n",
    "    y_s = safe_savgol(y, SMOOTH_WINDOW, SMOOTH_POLYORDER)\n",
    "    base = baseline_als(y_s, lam=3e5, p=0.01, niter=10)\n",
    "    y_bc = np.clip(y_s - base, 0, None)\n",
    "\n",
    "    noise = estimate_noise_mad(y_bc)\n",
    "    peaks, props = find_peaks(y_bc, prominence=max(50.0, 5.0*noise), height=max(50.0, 5.0*noise))\n",
    "    out = []\n",
    "    for idx, prom, h in zip(peaks, props[\"prominences\"], props[\"peak_heights\"]):\n",
    "        rp = fractional_peak_pos(x, y_bc, int(idx))\n",
    "        out.append((rp, float(prom), float(h)))\n",
    "    out.sort(key=lambda t: t[0])\n",
    "    return out\n",
    "\n",
    "def _score_assignment(raw_peaks: np.ndarray, exp_sizes: np.ndarray, a: float, b: float) -> Tuple[int, float]:\n",
    "    \"\"\"\n",
    "    Score: how many peaks hit and RMSE (in bp) after prediction via linear map size=a*raw+b.\n",
    "    \"\"\"\n",
    "    pred = a*raw_peaks + b\n",
    "    matched = []\n",
    "    used = set()\n",
    "    for s in exp_sizes:\n",
    "        j = int(np.argmin(np.abs(pred - s)))\n",
    "        if j in used:\n",
    "            continue\n",
    "        if abs(pred[j] - s) <= 6.0:  \n",
    "            used.add(j)\n",
    "            matched.append((pred[j] - s)**2)\n",
    "    if not matched:\n",
    "        return 0, float(\"inf\")\n",
    "    rmse = math.sqrt(float(np.mean(matched)))\n",
    "    return len(matched), rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add352fe",
   "metadata": {},
   "source": [
    "## Size calibration using LIZ500 (channel 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be1a9d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate_axis_liz500(raw_x: np.ndarray, y5: np.ndarray) -> Optional[PchipInterpolator]:\n",
    "    \"\"\"\n",
    "    Calibra eixo via LIZ500 com busca de âncoras + scoring (robusto a shift/extra/missing peaks).\n",
    "    Retorna interpolador raw_x -> bp.\n",
    "    \"\"\"\n",
    "    ladder_peaks = _detect_ladder_peaks(raw_x, y5)\n",
    "    if len(ladder_peaks) < 6:\n",
    "        return None\n",
    "\n",
    "    # uses top N per height to reduce spikes\n",
    "    peaks_arr = np.array(ladder_peaks, dtype=float)  #\n",
    "    # pick ups to  28 higher peaks and reorder by raw_pos\n",
    "    top = peaks_arr[np.argsort(peaks_arr[:, 2])[-min(28, len(peaks_arr)) :]]\n",
    "    top = top[np.argsort(top[:, 0])]\n",
    "    raw_peaks = top[:, 0]\n",
    "\n",
    "    best = None  # (matches, rmse, a, b)\n",
    "    obs_idx = list(range(len(raw_peaks)))\n",
    "    exp_idx = list(range(len(LIZ_500_SIZES)))\n",
    "    anchor_obs = sorted(set([0, 1, 2, len(obs_idx)-3, len(obs_idx)-2, len(obs_idx)-1] + list(range(4, len(obs_idx)-4, max(1, len(obs_idx)//6)))))\n",
    "    anchor_exp = [0, 1, 2, 3, 4, 6, 8, 10, 12, 14, 15]\n",
    "\n",
    "    for i in anchor_obs:\n",
    "        for j in anchor_obs:\n",
    "            if j <= i:\n",
    "                continue\n",
    "            x1, x2 = raw_peaks[i], raw_peaks[j]\n",
    "            if x2 - x1 < 5:\n",
    "                continue\n",
    "            for a_i in anchor_exp:\n",
    "                for a_j in anchor_exp:\n",
    "                    if a_j <= a_i:\n",
    "                        continue\n",
    "                    s1, s2 = LIZ_500_SIZES[a_i], LIZ_500_SIZES[a_j]\n",
    "                    a = (s2 - s1) / (x2 - x1)\n",
    "                    if a <= 0:\n",
    "                        continue\n",
    "                    b = s1 - a * x1\n",
    "                    mcount, rmse = _score_assignment(raw_peaks, LIZ_500_SIZES, a, b)\n",
    "                    if mcount < 8:\n",
    "                        continue\n",
    "                    cand = (mcount, rmse, a, b)\n",
    "                    if (best is None) or (cand[0] > best[0]) or (cand[0] == best[0] and cand[1] < best[1]):\n",
    "                        best = cand\n",
    "\n",
    "    if best is None:\n",
    "        return None\n",
    "\n",
    "    mcount, rmse, a, b = best\n",
    "    pred = a*raw_peaks + b\n",
    "\n",
    "    # builds monotonic pairs by choosing the raw whose pred is closest to each size\n",
    "    pairs = []\n",
    "    used = set()\n",
    "    for s in LIZ_500_SIZES:\n",
    "        j = int(np.argmin(np.abs(pred - s)))\n",
    "        if j in used:\n",
    "            continue\n",
    "        if abs(pred[j] - s) <= 6.0:\n",
    "            used.add(j)\n",
    "            pairs.append((float(raw_peaks[j]), float(s)))\n",
    "\n",
    "    # guarantees monotonicity and minimums\n",
    "    pairs.sort(key=lambda t: t[0])\n",
    "    if len(pairs) < 8:\n",
    "        return None\n",
    "\n",
    "    raw_knots = np.array([p[0] for p in pairs], dtype=float)\n",
    "    size_knots = np.array([p[1] for p in pairs], dtype=float)\n",
    "\n",
    "    # removes duplicated knots \n",
    "    keep = [0]\n",
    "    for k in range(1, len(raw_knots)):\n",
    "        if raw_knots[k] - raw_knots[keep[-1]] > 1e-6:\n",
    "            keep.append(k)\n",
    "    raw_knots = raw_knots[keep]\n",
    "    size_knots = size_knots[keep]\n",
    "\n",
    "    try:\n",
    "        return PchipInterpolator(raw_knots, size_knots, extrapolate=True)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def make_inverse_mapper(raw_x: np.ndarray, raw2bp: PchipInterpolator) -> PchipInterpolator:\n",
    "    \"\"\"Builds a bp->raw inverse mapper from a monotonic raw->bp mapper..\n",
    "    \"\"\"\n",
    "    raw_x = np.asarray(raw_x, dtype=float)\n",
    "    idx = np.argsort(raw_x)\n",
    "    rx = raw_x[idx]\n",
    "    bp = np.asarray(raw2bp(rx), dtype=float)\n",
    "    j = np.argsort(bp)\n",
    "    bp_s = bp[j]\n",
    "    rx_s = rx[j]\n",
    "    keep = np.concatenate([[True], np.abs(np.diff(bp_s)) > 1e-6])\n",
    "    bp_s = bp_s[keep]\n",
    "    rx_s = rx_s[keep]\n",
    "    if bp_s.size < 2:\n",
    "        return PchipInterpolator([0.0, 1.0], [0.0, 1.0], extrapolate=True)\n",
    "    return PchipInterpolator(bp_s, rx_s, extrapolate=True)\n",
    "\n",
    "\n",
    "# PULL-UP / BLEED CORRECTION (LADDER -> DYES)\n",
    "def subtract_ladder_bleed(raw_x: np.ndarray,\n",
    "                          ch5_raw: np.ndarray,\n",
    "                          dye_channels_raw: Dict[int, np.ndarray],\n",
    "                          bp2raw: Optional[PchipInterpolator],\n",
    "                          ladder_sizes: np.ndarray,\n",
    "                          *,\n",
    "                          use_sizes_min: float = 100.0,\n",
    "                          use_sizes_max: float = 500.0,\n",
    "                          window_samples: int = 6,\n",
    "                          min_ladder_rfu: float = 50.0,\n",
    "                          robust_quantile: float = 0.60) -> Tuple[Dict[int, np.ndarray], Dict[int, float]]:\n",
    "    \"\"\"Removes bleed/pull-up from channel 5 (ladder) nos in dyes channels (1..4) \n",
    "    \"\"\"\n",
    "    raw_x = np.asarray(raw_x, dtype=float)\n",
    "    ch5 = np.asarray(ch5_raw, dtype=float)\n",
    "    n = raw_x.size\n",
    "    if n == 0 or bp2raw is None:\n",
    "        return {k: np.asarray(v, dtype=float) for k, v in dye_channels_raw.items()}, {k: 0.0 for k in dye_channels_raw}\n",
    "\n",
    "    # select useful sizes  ( <100 \n",
    "    sizes = np.asarray(ladder_sizes, dtype=float)\n",
    "    sizes = sizes[(sizes >= use_sizes_min) & (sizes <= use_sizes_max)]\n",
    "    if sizes.size < 3:\n",
    "        return {k: np.asarray(v, dtype=float) for k, v in dye_channels_raw.items()}, {k: 0.0 for k in dye_channels_raw}\n",
    "\n",
    "    # expected raw positions by size\n",
    "    try:\n",
    "        raw_targets = np.asarray(bp2raw(sizes), dtype=float)\n",
    "    except Exception:\n",
    "        return {k: np.asarray(v, dtype=float) for k, v in dye_channels_raw.items()}, {k: 0.0 for k in dye_channels_raw}\n",
    "\n",
    "    # indexes approxtimated through searchsorted \n",
    "    order = np.argsort(raw_x)\n",
    "    rx_sorted = raw_x[order]\n",
    "\n",
    "    def nearest_idx(rt: float) -> int:\n",
    "        j = int(np.searchsorted(rx_sorted, rt))\n",
    "        if j <= 0:\n",
    "            ii = 0\n",
    "        elif j >= rx_sorted.size:\n",
    "            ii = rx_sorted.size - 1\n",
    "        else:\n",
    "            ii = j-1 if abs(rx_sorted[j-1] - rt) <= abs(rx_sorted[j] - rt) else j\n",
    "        return int(order[ii])\n",
    "\n",
    "    idxs = [nearest_idx(float(rt)) for rt in raw_targets]\n",
    "\n",
    "    # measure local ladder\n",
    "    ladder_local = []\n",
    "    idxs_ok = []\n",
    "    for ii in idxs:\n",
    "        lo = max(0, ii - window_samples)\n",
    "        hi = min(n, ii + window_samples + 1)\n",
    "        lv = float(np.max(ch5[lo:hi])) if hi > lo else float(ch5[ii])\n",
    "        if lv >= min_ladder_rfu:\n",
    "            ladder_local.append(lv)\n",
    "            idxs_ok.append(ii)\n",
    "\n",
    "    if len(idxs_ok) < 4:\n",
    "        # \n",
    "        return {k: np.asarray(v, dtype=float) for k, v in dye_channels_raw.items()}, {k: 0.0 for k in dye_channels_raw}\n",
    "\n",
    "    ladder_local = np.asarray(ladder_local, dtype=float)\n",
    "    thr = float(np.quantile(ladder_local, robust_quantile))\n",
    "    idxs_good = [ii for ii, lv in zip(idxs_ok, ladder_local) if lv >= thr]\n",
    "    if len(idxs_good) < 3:\n",
    "        idxs_good = idxs_ok\n",
    "\n",
    "    alphas: Dict[int, float] = {}\n",
    "    corrected: Dict[int, np.ndarray] = {}\n",
    "\n",
    "    for ch, yraw in dye_channels_raw.items():\n",
    "        y = np.asarray(yraw, dtype=float)\n",
    "        ratios = []\n",
    "        for ii in idxs_good:\n",
    "            lo = max(0, ii - window_samples)\n",
    "            hi = min(n, ii + window_samples + 1)\n",
    "            lv = float(np.max(ch5[lo:hi])) if hi > lo else float(ch5[ii])\n",
    "            if lv < min_ladder_rfu:\n",
    "                continue\n",
    "            dv = float(np.max(y[lo:hi])) if hi > lo else float(y[ii])\n",
    "            ratios.append(dv / (lv + 1e-9))\n",
    "        if len(ratios) >= 3:\n",
    "            a = float(np.median(ratios))\n",
    "        else:\n",
    "            a = 0.0\n",
    "        alphas[ch] = a\n",
    "        yc = y - a * ch5\n",
    "        corrected[ch] = np.clip(yc, 0, None)\n",
    "\n",
    "    return corrected, alphas\n",
    "\n",
    "# PEAK DETECTION (ALLOS)\n",
    "\n",
    "def detect_peaks_subpixel(raw_x: np.ndarray, y: np.ndarray, interp: PchipInterpolator) -> List[PeakCandidate]:\n",
    "    mask = (raw_x >= X_MIN_ALLELES) & (raw_x <= X_MAX)\n",
    "    x = raw_x[mask]\n",
    "    sig = y[mask].astype(float)\n",
    "\n",
    "    # smoothing\n",
    "    sig_s = safe_savgol(sig, SMOOTH_WINDOW, SMOOTH_POLYORDER)\n",
    "\n",
    "    # baseline AsLS \n",
    "    base = baseline_als(sig_s, lam=8e5, p=0.001, niter=10)\n",
    "    sig_bc = np.clip(sig_s - base, 0, None)\n",
    "\n",
    "    noise = estimate_noise_mad(sig_bc)\n",
    "    min_prom = max(MIN_PROM_ABS, PROM_SNR * noise)\n",
    "    min_h = max(MIN_HEIGHT_ABS, PROM_SNR * noise)\n",
    "\n",
    "    peaks, props = find_peaks(sig_bc, prominence=min_prom, height=min_h, width=MIN_PEAK_WIDTH_SAMPLES)\n",
    "    out = []\n",
    "    for idx, prom, h in zip(peaks, props[\"prominences\"], props[\"peak_heights\"]):\n",
    "        rp = fractional_peak_pos(x, sig_bc, int(idx))\n",
    "        bp = float(interp(rp))\n",
    "        out.append(PeakCandidate(pos=bp, raw_pos=rp, prom=float(prom), height=float(h)))\n",
    "\n",
    "    out.sort(key=lambda p: p.pos)\n",
    "    return out\n",
    "\n",
    "# CROSSTALK SUPPRESSION\n",
    "\n",
    "def suppress_crosstalk(marker_peaks: Dict[int, List[PeakCandidate]]) -> Dict[int, List[PeakCandidate]]:\n",
    "    all_peaks = []\n",
    "    for m_id, peaks in marker_peaks.items():\n",
    "        for p in peaks:\n",
    "            all_peaks.append((m_id, p))\n",
    "    to_remove = set()\n",
    "    for i in range(len(all_peaks)):\n",
    "        m1, p1 = all_peaks[i]\n",
    "        for j in range(i+1, len(all_peaks)):\n",
    "            m2, p2 = all_peaks[j]\n",
    "            if abs(p1.pos - p2.pos) <= CROSSTALK_TOL_PB:\n",
    "                if p1.height >= p2.height * CROSSTALK_STRONG_RATIO:\n",
    "                    to_remove.add((m2, p2.raw_pos))\n",
    "                elif p2.height >= p1.height * CROSSTALK_STRONG_RATIO:\n",
    "                    to_remove.add((m1, p1.raw_pos))\n",
    "    cleaned = {}\n",
    "    for m_id, peaks in marker_peaks.items():\n",
    "        cleaned[m_id] = [p for p in peaks if (m_id, p.raw_pos) not in to_remove]\n",
    "    return cleaned\n",
    "\n",
    "# PANEL SNAP + STUTTER (PROTECTED)\n",
    "\n",
    "def snap_to_panel(peaks: List[PeakCandidate], panel_sizes: List[float]) -> List[MappedPeak]:\n",
    "    if not peaks or not panel_sizes:\n",
    "        return []\n",
    "    panel = np.array(panel_sizes, dtype=float)\n",
    "    mapped = []\n",
    "    for p in peaks:\n",
    "        j = int(np.argmin(np.abs(panel - p.pos)))\n",
    "        if abs(panel[j] - p.pos) <= PANEL_SNAP_TOL_PB:\n",
    "            mapped.append(MappedPeak(allele_size=float(panel[j]),\n",
    "                                     calibrated_pos=float(p.pos),\n",
    "                                     raw_pos=float(p.raw_pos),\n",
    "                                     prom=float(p.prom),\n",
    "                                     height=float(p.height)))\n",
    "    # groups by allele_size keepong the better one  (bigger prom)\n",
    "    best = {}\n",
    "    for mp in mapped:\n",
    "        k = mp.allele_size\n",
    "        if (k not in best) or (mp.prom > best[k].prom):\n",
    "            best[k] = mp\n",
    "    out = list(best.values())\n",
    "    out.sort(key=lambda m: m.allele_size)\n",
    "    return out\n",
    "\n",
    "def filter_stutters_panel_protected(mapped: List[MappedPeak]) -> List[MappedPeak]:\n",
    "    if len(mapped) <= 1:\n",
    "        return mapped\n",
    "\n",
    "    mapped = sorted(mapped, key=lambda m: m.allele_size)\n",
    "    keep = [True]*len(mapped)\n",
    "\n",
    "    protected_pairs = set()\n",
    "    for i in range(len(mapped)):\n",
    "        for j in range(i+1, len(mapped)):\n",
    "            d = mapped[j].allele_size - mapped[i].allele_size\n",
    "            if d <= HET_PROTECT_PB:\n",
    "                protected_pairs.add((i, j))\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    for i in range(len(mapped)):\n",
    "        if not keep[i]:\n",
    "            continue\n",
    "        for j in range(len(mapped)):\n",
    "            if i == j or not keep[j]:\n",
    "                continue\n",
    "            a, b = (i, j) if i < j else (j, i)\n",
    "            if (a, b) in protected_pairs:\n",
    "                continue\n",
    "\n",
    "            delta = mapped[j].allele_size - mapped[i].allele_size\n",
    "            if STUTTER_WINDOW[0] <= delta <= STUTTER_WINDOW[1]:\n",
    "                if mapped[i].height > mapped[j].height and (mapped[j].height / (mapped[i].height + 1e-9)) <= STUTTER_MAX_RATIO:\n",
    "                    keep[j] = False\n",
    "\n",
    "    return [m for m, k in zip(mapped, keep) if k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cc8d6d",
   "metadata": {},
   "source": [
    "## Allele calling (stutter handling, selection, export helpers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b4e5c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_alleles(mapped: List[MappedPeak]) -> Tuple[int, Optional[MappedPeak], Optional[MappedPeak]]:\n",
    "    if not mapped:\n",
    "        return 0, None, None\n",
    "    mapped = sorted(mapped, key=lambda m: (m.score, m.height), reverse=True)\n",
    "    a1 = mapped[0]\n",
    "    a2 = None\n",
    "    if len(mapped) > 1:\n",
    "        for cand in mapped[1:]:\n",
    "            if abs(cand.allele_size - a1.allele_size) >= MIN_ALLELE_SEP_PB and cand.height >= a1.height * ALLELE2_MIN_RATIO:\n",
    "                a2 = cand\n",
    "                break\n",
    "    cnt = 1 if a1 is not None else 0\n",
    "    if a2 is not None:\n",
    "        cnt = 2\n",
    "        # ordena por size\n",
    "        if a2.allele_size < a1.allele_size:\n",
    "            a1, a2 = a2, a1\n",
    "    return cnt, a1, a2\n",
    "\n",
    "# IO / WORKERS\n",
    "def _read_raw_csv_from_zip(zf: zipfile.ZipFile, member: str) -> pd.DataFrame:\n",
    "    with zf.open(member, \"r\") as f:\n",
    "        df = pd.read_csv(f, sep=\";\")\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def _worker_process_plant(plant_id: int, panel: Dict[int, List[float]]) -> Dict:\n",
    "    with zipfile.ZipFile(ZIP_PATH, \"r\") as zf:\n",
    "        base = get_zip_base_dir(zf)\n",
    "        prefix = f\"{base}/\" if base else \"\"\n",
    "        m1_name = f\"{prefix}M1_pl{plant_id}.csv\"\n",
    "        m2_name = f\"{prefix}M2_pl{plant_id}.csv\"\n",
    "\n",
    "        df1 = _read_raw_csv_from_zip(zf, m1_name)\n",
    "        df2 = _read_raw_csv_from_zip(zf, m2_name)\n",
    "\n",
    "    # raw axe\n",
    "    raw_x1 = df1[\"molw\"].to_numpy(dtype=float)\n",
    "    raw_x2 = df2[\"molw\"].to_numpy(dtype=float)\n",
    "\n",
    "    # calibration by  multiplex (through channel 5)\n",
    "    interp1 = calibrate_axis_liz500(raw_x1, df1[\"channel_5\"].to_numpy())\n",
    "    interp2 = calibrate_axis_liz500(raw_x2, df2[\"channel_5\"].to_numpy())\n",
    "\n",
    "    if interp1 is None:\n",
    "        interp1 = PchipInterpolator([raw_x1.min(), raw_x1.max()], [raw_x1.min(), raw_x1.max()], extrapolate=True)\n",
    "    if interp2 is None:\n",
    "        interp2 = PchipInterpolator([raw_x2.min(), raw_x2.max()], [raw_x2.min(), raw_x2.max()], extrapolate=True)\n",
    "\n",
    "\n",
    "    inv1 = make_inverse_mapper(raw_x1, interp1)\n",
    "    inv2 = make_inverse_mapper(raw_x2, interp2)\n",
    "    ch5_1 = df1[\"channel_5\"].to_numpy(dtype=float)\n",
    "    ch5_2 = df2[\"channel_5\"].to_numpy(dtype=float)\n",
    "\n",
    "    m1_raw_channels = {1: df1[\"channel_1\"].to_numpy(dtype=float),\n",
    "                       2: df1[\"channel_2\"].to_numpy(dtype=float),\n",
    "                       3: df1[\"channel_3\"].to_numpy(dtype=float),\n",
    "                       4: df1[\"channel_4\"].to_numpy(dtype=float)}\n",
    "    m2_raw_channels = {1: df2[\"channel_1\"].to_numpy(dtype=float),\n",
    "                       2: df2[\"channel_2\"].to_numpy(dtype=float),\n",
    "                       3: df2[\"channel_3\"].to_numpy(dtype=float),\n",
    "                       4: df2[\"channel_4\"].to_numpy(dtype=float)}\n",
    "\n",
    "    m1_corr_channels, m1_alphas = subtract_ladder_bleed(raw_x1, ch5_1, m1_raw_channels, inv1, LIZ_500_SIZES)\n",
    "    m2_corr_channels, m2_alphas = subtract_ladder_bleed(raw_x2, ch5_2, m2_raw_channels, inv2, LIZ_500_SIZES)\n",
    "\n",
    "    # pre processing for score\n",
    "    m1_bc = {}\n",
    "    for mid, (mx, ch) in MARKER_MAP.items():\n",
    "        if mx != 1 or mid > 4:\n",
    "            continue\n",
    "        x_f, bc = _preprocess_for_scoring(raw_x1, m1_corr_channels[ch], X_MIN_ALLELES)\n",
    "        m1_bc[mid] = (x_f, bc)\n",
    "    l1_bc = _preprocess_for_scoring(raw_x1, df1[\"channel_5\"].to_numpy(), X_MIN_LADDER)\n",
    "    \n",
    "    m2_bc = {}\n",
    "    for mid, (mx, ch) in MARKER_MAP.items():\n",
    "        if mx != 2 or mid < 5:\n",
    "            continue\n",
    "        x_f, bc = _preprocess_for_scoring(raw_x2, m2_corr_channels[ch], X_MIN_ALLELES)\n",
    "        m2_bc[mid] = (x_f, bc)\n",
    "    l2_bc = _preprocess_for_scoring(raw_x2, df2[\"channel_5\"].to_numpy(), X_MIN_LADDER)\n",
    "    \n",
    "    marker_peaks = {}\n",
    "    for marker_id, (mx, ch) in MARKER_MAP.items():\n",
    "        if mx == 1:\n",
    "            y = m1_corr_channels[ch]\n",
    "            marker_peaks[marker_id] = detect_peaks_subpixel(raw_x1, y, interp1)\n",
    "        else:\n",
    "            y = m2_corr_channels[ch]\n",
    "            marker_peaks[marker_id] = detect_peaks_subpixel(raw_x2, y, interp2)\n",
    "\n",
    "    # crosstalk \n",
    "    marker_peaks = suppress_crosstalk(marker_peaks)\n",
    "\n",
    "\n",
    "    # snap + stutter protected\n",
    "    calls = {}\n",
    "    for marker_id in range(1, 9):\n",
    "        mapped = snap_to_panel(marker_peaks.get(marker_id, []), panel.get(marker_id, []))\n",
    "\n",
    "        if marker_id <= 4:\n",
    "            mapped = _compute_scores_for_mapped(marker_id, mapped, m1_bc, l1_bc, bp2raw=inv1)\n",
    "        else:\n",
    "            mapped = _compute_scores_for_mapped(marker_id, mapped, m2_bc, l2_bc, bp2raw=inv2)\n",
    "\n",
    "        mapped = filter_stutters_panel_protected(mapped)\n",
    "        allele_count, a1, a2 = call_alleles(mapped)\n",
    "        calls[marker_id] = (allele_count, a1, a2)\n",
    "\n",
    "    return {\"plant_id\": plant_id, \"calls\": calls}\n",
    "\n",
    "\n",
    "\n",
    "def generate_plot(plant_id: int, df: pd.DataFrame, interp: PchipInterpolator, out_path: Path):\n",
    "    raw_x = df[\"molw\"].to_numpy(dtype=float)\n",
    "    x = raw_x\n",
    "    true_x = interp(x)\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.plot(true_x, df[\"channel_1\"], label=\"Ch1\")\n",
    "    ax.plot(true_x, df[\"channel_2\"], label=\"Ch2\")\n",
    "    ax.plot(true_x, df[\"channel_3\"], label=\"Ch3\")\n",
    "    ax.plot(true_x, df[\"channel_4\"], label=\"Ch4\")\n",
    "    ax.plot(true_x, df[\"channel_5\"], label=\"Ladder (Ch5)\", alpha=0.8)\n",
    "    ax.set_title(f\"Plant {plant_id} - Multiplex (calibrated)\")\n",
    "    ax.set_xlabel(\"True size (bp)\")\n",
    "    ax.set_ylabel(\"RFU\")\n",
    "    ax.set_xlim(0, X_MAX)\n",
    "    ax.legend()\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_path, dpi=140)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23af22d6",
   "metadata": {},
   "source": [
    "## Run classifier + optional validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a99c89f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encontradas 384 plantas no zip.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chamando alelos (EC): 100%|█████████████████| 384/384 [03:30<00:00,  1.83planta/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Classificação concluída.\n",
      "Saídas:\n",
      " - outputs_v20_4/tables/miRNA_allele_calls_wide.csv\n",
      " - outputs_v20_4/tables/miRNA_allele_calls_long.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    TABLES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    panel = load_and_build_panel(LABEL_DIR)\n",
    "\n",
    "    plants = list_plants_in_zip(ZIP_PATH)\n",
    "    print(f\"Encontradas {len(plants)} plantas no zip.\")\n",
    "\n",
    "    results = []\n",
    "    if RUN_PARALLEL:\n",
    "        with concurrent.futures.ProcessPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "            futs = [ex.submit(_worker_process_plant, pid, panel) for pid in plants]\n",
    "            with tqdm(total=len(futs), desc=\"Chamando alelos (EC)\", unit=\"planta\") as pbar:\n",
    "                for f in concurrent.futures.as_completed(futs):\n",
    "                    results.append(f.result())\n",
    "                    pbar.update(1)\n",
    "    else:\n",
    "        for pid in tqdm(plants, desc=\"Chamando alelos (EC)\", unit=\"planta\"):\n",
    "            results.append(_worker_process_plant(pid, panel))\n",
    "\n",
    "    results = sorted(results, key=lambda d: d[\"plant_id\"])\n",
    "\n",
    "    rows = []\n",
    "    for r in results:\n",
    "        row = {\"plant_id\": int(r[\"plant_id\"])}\n",
    "        for m_id in range(1, 9):\n",
    "            allele_count, a1, a2 = r[\"calls\"][m_id]\n",
    "            prefix = f\"miR{m_id}\"\n",
    "            if DUPLICATE_HOMOZYGOUS and (a1 is not None) and (a2 is None) and (allele_count == 1):\n",
    "                a2 = a1\n",
    "                if HOMO_AS_TWO_ALLELES:\n",
    "                    allele_count = 2\n",
    "\n",
    "            row[f\"{prefix}_allele_count\"] = int(allele_count)\n",
    "            row[f\"{prefix}_a1_size\"] = np.nan if a1 is None else float(a1.allele_size)\n",
    "            row[f\"{prefix}_a2_size\"] = np.nan if a2 is None else float(a2.allele_size)\n",
    "            row[f\"{prefix}_a1_raw_pos\"] = np.nan if a1 is None else float(a1.calibrated_pos)\n",
    "            row[f\"{prefix}_a2_raw_pos\"] = np.nan if a2 is None else float(a2.calibrated_pos)\n",
    "            row[f\"{prefix}_a1_prom\"] = np.nan if a1 is None else float(a1.prom)\n",
    "            row[f\"{prefix}_a2_prom\"] = np.nan if a2 is None else float(a2.prom)\n",
    "            row[f\"{prefix}_present\"] = int(allele_count > 0)\n",
    "        rows.append(row)\n",
    "\n",
    "    wide = pd.DataFrame(rows)\n",
    "    wide.to_csv(TABLES_DIR / \"miRNA_allele_calls_wide.csv\", index=False)\n",
    "\n",
    "    # builds the \"long\"output\n",
    "    long_rows = []\n",
    "    for r in results:\n",
    "        pid = int(r[\"plant_id\"])\n",
    "        for m_id in range(1, 9):\n",
    "            allele_count, a1, a2 = r[\"calls\"][m_id]\n",
    "            if allele_count == 0:\n",
    "                long_rows.append({\"plant_id\": pid, \"marker_id\": m_id, \"allele_rank\": 0,\n",
    "                                  \"allele_size\": np.nan, \"calibrated_pos\": np.nan, \"prom\": np.nan, \"height\": np.nan})\n",
    "            else:\n",
    "                if DUPLICATE_HOMOZYGOUS and (a1 is not None) and (a2 is None) and (allele_count == 1):\n",
    "                    a2 = a1\n",
    "                    if HOMO_AS_TWO_ALLELES:\n",
    "                        allele_count = 2\n",
    "\n",
    "                for rank, a in [(1, a1), (2, a2)]:\n",
    "                    if a is None: \n",
    "                        continue\n",
    "                    long_rows.append({\"plant_id\": pid, \"marker_id\": m_id, \"allele_rank\": rank,\n",
    "                                      \"allele_size\": float(a.allele_size),\n",
    "                                      \"calibrated_pos\": float(a.calibrated_pos),\n",
    "                                      \"prom\": float(a.prom),\n",
    "                                      \"height\": float(a.height)})\n",
    "    long_df = pd.DataFrame(long_rows)\n",
    "    long_df.to_csv(TABLES_DIR / \"miRNA_allele_calls_long.csv\", index=False)\n",
    "\n",
    "    print(\" Classification ended.\")\n",
    "    print(\"Saídas:\")\n",
    "    print(\" -\", TABLES_DIR / \"miRNA_allele_calls_wide.csv\")\n",
    "    print(\" -\", TABLES_DIR / \"miRNA_allele_calls_long.csv\")\n",
    "\n",
    "# PREPROCESSING FOR  SCORE (anti-artefacts)\n",
    "def _preprocess_for_scoring(raw_x: np.ndarray, y: np.ndarray, x_min: float) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    mask = (raw_x >= x_min) & (raw_x <= X_MAX)\n",
    "    x = raw_x[mask].astype(float)\n",
    "    sig = y[mask].astype(float)\n",
    "    sig_s = safe_savgol(sig, SMOOTH_WINDOW, SMOOTH_POLYORDER)\n",
    "    base = baseline_als(sig_s, lam=8e5 if x_min >= 100 else 5e5, p=0.001, niter=10)\n",
    "    sig_bc = np.clip(sig_s - base, 0, None)\n",
    "    return x, sig_bc\n",
    "\n",
    "def _local_max(x: np.ndarray, sig_bc: np.ndarray, raw_pos: float, w: int = MULTI_WINDOW_SAMPLES) -> float:\n",
    "    if len(x) == 0:\n",
    "        return 0.0\n",
    "    idx = int(np.argmin(np.abs(x - raw_pos)))\n",
    "    lo = max(0, idx - w)\n",
    "    hi = min(len(sig_bc), idx + w + 1)\n",
    "    return float(np.max(sig_bc[lo:hi])) if hi > lo else float(sig_bc[idx])\n",
    "\n",
    "\n",
    "def _compute_scores_for_mapped(marker_id: int,\n",
    "                              mapped: List[MappedPeak],\n",
    "                              sig_bc_channels: Dict[int, Tuple[np.ndarray, np.ndarray]],\n",
    "                              ladder_bc: Tuple[np.ndarray, np.ndarray],\n",
    "                              bp2raw: Optional[PchipInterpolator] = None) -> List[MappedPeak]:\n",
    "    if not mapped:\n",
    "        return mapped\n",
    "\n",
    "    lx, lbc = ladder_bc\n",
    "    ladder_ref = float(np.quantile(lbc, LADDER_STRONG_Q)) + 1e-9\n",
    "\n",
    "    def raw_at_bp(bp: float, fallback_raw: float) -> float:\n",
    "        if bp2raw is None:\n",
    "            return fallback_raw\n",
    "        try:\n",
    "            v = float(bp2raw(bp))\n",
    "            if math.isfinite(v):\n",
    "                return v\n",
    "        except Exception:\n",
    "            pass\n",
    "        return fallback_raw\n",
    "\n",
    "    for mp in mapped:\n",
    "        rpos = raw_at_bp(mp.allele_size, mp.raw_pos)\n",
    "\n",
    "        x_main, bc_main = sig_bc_channels[marker_id]\n",
    "        main = _local_max(x_main, bc_main, rpos)\n",
    "\n",
    "        other = 0.0\n",
    "        for mid in sig_bc_channels.keys():\n",
    "            if mid == marker_id:\n",
    "                continue\n",
    "            xo, bco = sig_bc_channels[mid]\n",
    "            other += _local_max(xo, bco, rpos)\n",
    "\n",
    "        multi_ratio = other / (main + 1e-9)\n",
    "\n",
    "        lad = _local_max(lx, lbc, rpos)\n",
    "        near_liz = float(np.min(np.abs(LIZ_500_SIZES - mp.allele_size))) <= LADDER_MATCH_PB\n",
    "        ladder_strong = (lad / ladder_ref) >= 0.5\n",
    "        ladder_ratio = main / (lad + 1e-9)\n",
    "\n",
    "        flags = []\n",
    "        score = float(mp.prom)\n",
    "\n",
    "        if marker_id == 2 and abs(mp.allele_size - 250.0) <= LADDER_MATCH_PB and ladder_strong and ladder_ratio < 0.20:\n",
    "            mp.score = 0.0\n",
    "            mp.multi_ratio = float(multi_ratio)\n",
    "            mp.ladder_ratio = float(ladder_ratio)\n",
    "            mp.flags = \"BLACKLIST250\"\n",
    "            continue\n",
    "\n",
    "        if marker_id == 2 and abs(mp.allele_size - 148.0) <= LADDER_MATCH_PB and multi_ratio >= 0.25:\n",
    "            mp.score = 0.0\n",
    "            mp.multi_ratio = float(multi_ratio)\n",
    "            mp.ladder_ratio = float(ladder_ratio)\n",
    "            mp.flags = \"BLACKLIST148\"\n",
    "            continue\n",
    "\n",
    "        # multichannel penalisation\n",
    "        if multi_ratio > MULTI_RATIO_MAX:\n",
    "            flags.append(\"MULTI\")\n",
    "            score *= max(0.02, 1.0 - min(1.0, multi_ratio))\n",
    "            if multi_ratio >= MULTI_RATIO_HARD:\n",
    "                score *= 0.05\n",
    "\n",
    "        # penalisation ladder bleed (general)\n",
    "        if near_liz and ladder_strong and ladder_ratio < LADDER_BLEED_RATIO_MAX:\n",
    "            flags.append(\"LADDER_BLEED\")\n",
    "            score *= 0.03\n",
    "\n",
    "        mp.score = float(score)\n",
    "        mp.multi_ratio = float(multi_ratio)\n",
    "        mp.ladder_ratio = float(ladder_ratio)\n",
    "        mp.flags = \",\".join(flags)\n",
    "\n",
    "    return mapped\n",
    "\n",
    "\n",
    "    # reference ladder \n",
    "    lx, lbc = ladder_bc\n",
    "    ladder_ref = float(np.quantile(lbc, LADDER_STRONG_Q)) + 1e-9\n",
    "\n",
    "    for mp in mapped:\n",
    "        x_main, bc_main = sig_bc_channels[marker_id]\n",
    "        main = _local_max(x_main, bc_main, mp.raw_pos)\n",
    "        other = 0.0\n",
    "        for mid in sig_bc_channels.keys():\n",
    "            if mid == marker_id:\n",
    "                continue\n",
    "            xo, bco = sig_bc_channels[mid]\n",
    "            other += _local_max(xo, bco, mp.raw_pos)\n",
    "\n",
    "        multi_ratio = other / (main + 1e-9)\n",
    "\n",
    "        lad = _local_max(lx, lbc, mp.raw_pos)\n",
    "        near_liz = float(np.min(np.abs(LIZ_500_SIZES - mp.allele_size))) <= LADDER_MATCH_PB\n",
    "        ladder_strong = (lad / ladder_ref) >= 0.5  \n",
    "        ladder_ratio = main / (lad + 1e-9)\n",
    "\n",
    "        flags = []\n",
    "        score = float(mp.prom)\n",
    "\n",
    "        if multi_ratio > MULTI_RATIO_MAX:\n",
    "            flags.append(\"MULTI\")\n",
    "            score *= max(0.05, 1.0 - min(1.0, multi_ratio))  # penaliza progressivo\n",
    "            if multi_ratio >= MULTI_RATIO_HARD:\n",
    "                score *= 0.10\n",
    "\n",
    "        if near_liz and ladder_strong and ladder_ratio < LADDER_BLEED_RATIO_MAX:\n",
    "            flags.append(\"LADDER_BLEED\")\n",
    "            score *= 0.05\n",
    "\n",
    "        mp.score = float(score)\n",
    "        mp.multi_ratio = float(multi_ratio)\n",
    "        mp.ladder_ratio = float(ladder_ratio)\n",
    "        mp.flags = \",\".join(flags)\n",
    "\n",
    "    return mapped\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1470c773-3bf9-4ffc-bbf9-b71cd1c5a8bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
